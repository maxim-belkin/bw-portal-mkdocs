{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Blue Waters documentation Blue Waters portal: bluewaters.ncsa.illinois.edu","title":"Home"},{"location":"#blue-waters-documentation","text":"Blue Waters portal: bluewaters.ncsa.illinois.edu","title":"Blue Waters documentation"},{"location":"documentation/getting-started/","text":"Getting Started Guide This brief guide is intended to give fairly experienced users the basic information they need to get on the system, set up their environment, compile applications, and run batch jobs. A New User Webinar is also available for playback of video recordings. Contents Obtaining an Allocation Logging In Level of Expertise Expected of Blue Waters Users Transfering Files and Data Setting Up Your Environment Building Executables Running Batch Jobs Quick Hardware Summary []{#Allocation}Obtaining an Allocation There are several pathways available for researchers and educators to apply to use Blue Waters for their work. Visit the Allocations page . []{#Logging-duo}[]{#logging-in-duo}Logging In You may connect to Blue Waters via the external login hosts at bw.ncsa.illinois.edu using ssh with your NCSA DUO passcode or push response from your smartphone. This multi-factor authenication scheme provides significant security benefits. The bw.ncsa.illinois.edu address is a DNS roundrobin alias for h2ologin[1-4]. If you find that connecting to bw.ncsa.illinois.edu is not successful please try specifying a particular login host. For help activating your NCSA Duo account, reference this page . To check if your NCSA Duo is working proplerly, visit here . Depending on the choice you make there, you should receive a pass code or a push from Duo. Myproxy login and GSI enabled ssh are not supported with NCSA Duo authentication. []{#Expertise}Level of Expertise Expected for Blue Waters Users Most users of systems like Blue Waters have experience with other large high-performance computer systems. The instructions on this portal generally assume that the reader knows how to use a Unix-style command line, edit files, run (and modify) Makefiles to build code, write scripts, and submit jobs to a batch queue system. There are some things that work slightly differently on the Cray XE system than other systems; the portal documentation covers those in detail, but we assume that you know the basics already. If you\\'re not at that level yet (if you\\'re unfamiliar with things like ssh, emacs, vi, jpico, qsub, make, top) then you\\'ll need to gain some knowledge before you can use Blue Waters effectively. Here are a few links to resources that will teach you some of the basics about Unix command line tools and working on a high-performance computing system: https://www.xsede.org/web/xup/online-training https://newton.utk.edu/bin/view/Main/LinuxCommandLineBasics http://websistent.com/linux-acl-tutorial/ # explains linux Access Control Lists (ACL) compared with chmod []{#Transfer}Transfering Files and Data It is recommended that Globus Online (GO) is used for file transfers to and from Blue Waters. Blue Waters has dedicated import/export resources to provide superior I/O access to the filesystems. Most HPC centers provide GO endpoints and GO provides clients for desktop and laptop transfers. Please see the Data Transfer section of the User Guide . []{#Environment}Setting Up Your Environment The default shell is /bin/bash. You can change it by sending a request via email to help+bw\\@ncsa.illinois.edu. The user environment is controlled using the modules environment management system. Modules may be loaded, unloaded, or swapped either on a command line or in your \\$HOME/.bashrc (.cshrc for csh ) shell startup file. The command \\\"module avail\\\" will display the avail modules on the system. Please see the Programming Environment section of the User Guide. []{#Home_Directory}Home Directory Permissions By default, user home directories and /scratch directories are closed (permissions 700) with a parent directory setting that prevents users from opening up the permissions. See the File and Directory Access Control List page ( https://bluewaters.ncsa.illinois.edu/facl ) for Blue Waters file system policies. The /projects file system is designed as common space for your group; if you want a space that all your group members can access, that\\'s a good place for it. As always, your space on the /scratch file system is the best place for job inputs and outputs. []{#Building}Building Executables The compilers are defined in the PrgEnv module for each family of compilers. Invoke the ftn, cc, or CC (Fortran, C, or C++, respectively) commands after loading the appropriate programming environment, and the underlying compilers will be employed. To see the options specific to a compiler (pgi vs. cray) consult the man pages for the vendor\\'s compiler (man pgf90 or man crayftn). The correct xt-libsci providing the lapack and other libs for your programming environment is automatically included in the PrgEnv module you choose. The default programming environment (module PrgEnv-cray ) invokes the Cray compilers. PGI compilers are available via the PrgEnv-pgi module.Intel compilers are available via the PrgEnv-intel module. Gnu compilers are available via the PrgEnv-gnu module. OpenACC support is provided by both the Cray and PGI compilers. Please see the Compiling section of the User Guide for more information. []{#Running}Running Batch Jobs The batch environment is Torque/MOAB from Adaptive Computing which talk to the Cray\\'s Application Level Placement Scheduler (ALPS) to obtain resource information. The aprun utility is used to start jobs on compute nodes. Its closest analogs are mpirun or mpiexec as found on many commodity clusters. Unlike clusters, the use of aprun is mandatory on Blue Waters, which is not a Linux cluster but a massively parallel system (MPP), in order to start any jobs including non-MPI ones that run on a single node. If the PBS script does not use aprun to start the application the latter will start on a service node, which is a shared resource, and that will be a violation of the usage policy. If by any reason the single-node job does not properly run when invoked with help of aprun, the alternative is to run the single-node application in CCM-mode . aprun supports options that are unique to the Cray. There are flags to set process affinity by NUMA node, control thread placement, and set memory policy between NUMA nodes in a job. See the aprun man page for more details. Some common flags are highlighted and documented in the sample batch scripts. Example scripts are located in /sw/userdoc/samplescripts/ Following is a minimalistic PBS script to start a job app.exe running under GNU environment on two XE nodes #!/bin/bash #PBS -l nodes=2:ppn=32 #PBS -l walltime=00:30:00 #PBS -N testjob . /opt/modules/default/init/bash module swap PrgEnv-cray PrgEnv-gnu cd $PBS_O_WORKDIR aprun -n 64 ./app.exe < input.dat > output.out Once you have prepared a job script (called jobscript.pbs for example), you then use the qsub command to submit the job to the resource handling and job scheduling system. $ qsub jobscript.pbs The job is now in the queue (normal queue in this case as it was not specified). Please see the Running Your Jobs section of the User Guide for more information. []{#QuickHardwareSummary}Quick Hardware Summary The Blue Waters system is a Cray XE/XK hybrid machine composed of AMD 6276 \\\"Interlagos\\\" processors and NVIDIA GK110 \\\"Kepler\\\" accelerators all connected by the Cray Gemini torus interconnect. System Totals Cabinets 288 Peak Performance (XE+XK) 13.24 PF System Memory (XE+XK) 1.476 PB Compute Nodes (XE+XK) 26,864 Bulldozer Cores (XE+XK) 405,248 Kepler Accelerators 4,228 Torus Dimensions 24x24x24 Usable Disk Storage 26.4 PB Please see the detailed Hardware Summary page for more information.","title":"Getting Started"},{"location":"documentation/getting-started/#getting-started-guide","text":"This brief guide is intended to give fairly experienced users the basic information they need to get on the system, set up their environment, compile applications, and run batch jobs. A New User Webinar is also available for playback of video recordings.","title":"Getting Started Guide"},{"location":"documentation/getting-started/#contents","text":"Obtaining an Allocation Logging In Level of Expertise Expected of Blue Waters Users Transfering Files and Data Setting Up Your Environment Building Executables Running Batch Jobs Quick Hardware Summary","title":"Contents"},{"location":"documentation/getting-started/#allocationobtaining-an-allocation","text":"There are several pathways available for researchers and educators to apply to use Blue Waters for their work. Visit the Allocations page .","title":"[]{#Allocation}Obtaining an Allocation"},{"location":"documentation/getting-started/#logging-duologging-in-duologging-in","text":"You may connect to Blue Waters via the external login hosts at bw.ncsa.illinois.edu using ssh with your NCSA DUO passcode or push response from your smartphone. This multi-factor authenication scheme provides significant security benefits. The bw.ncsa.illinois.edu address is a DNS roundrobin alias for h2ologin[1-4]. If you find that connecting to bw.ncsa.illinois.edu is not successful please try specifying a particular login host. For help activating your NCSA Duo account, reference this page . To check if your NCSA Duo is working proplerly, visit here . Depending on the choice you make there, you should receive a pass code or a push from Duo. Myproxy login and GSI enabled ssh are not supported with NCSA Duo authentication.","title":"[]{#Logging-duo}[]{#logging-in-duo}Logging In"},{"location":"documentation/getting-started/#expertiselevel-of-expertise-expected-for-blue-waters-users","text":"Most users of systems like Blue Waters have experience with other large high-performance computer systems. The instructions on this portal generally assume that the reader knows how to use a Unix-style command line, edit files, run (and modify) Makefiles to build code, write scripts, and submit jobs to a batch queue system. There are some things that work slightly differently on the Cray XE system than other systems; the portal documentation covers those in detail, but we assume that you know the basics already. If you\\'re not at that level yet (if you\\'re unfamiliar with things like ssh, emacs, vi, jpico, qsub, make, top) then you\\'ll need to gain some knowledge before you can use Blue Waters effectively. Here are a few links to resources that will teach you some of the basics about Unix command line tools and working on a high-performance computing system: https://www.xsede.org/web/xup/online-training https://newton.utk.edu/bin/view/Main/LinuxCommandLineBasics http://websistent.com/linux-acl-tutorial/ # explains linux Access Control Lists (ACL) compared with chmod","title":"[]{#Expertise}Level of Expertise Expected for Blue Waters Users"},{"location":"documentation/getting-started/#transfertransfering-files-and-data","text":"It is recommended that Globus Online (GO) is used for file transfers to and from Blue Waters. Blue Waters has dedicated import/export resources to provide superior I/O access to the filesystems. Most HPC centers provide GO endpoints and GO provides clients for desktop and laptop transfers. Please see the Data Transfer section of the User Guide .","title":"[]{#Transfer}Transfering Files and Data"},{"location":"documentation/getting-started/#environmentsetting-up-your-environment","text":"The default shell is /bin/bash. You can change it by sending a request via email to help+bw\\@ncsa.illinois.edu. The user environment is controlled using the modules environment management system. Modules may be loaded, unloaded, or swapped either on a command line or in your \\$HOME/.bashrc (.cshrc for csh ) shell startup file. The command \\\"module avail\\\" will display the avail modules on the system. Please see the Programming Environment section of the User Guide.","title":"[]{#Environment}Setting Up Your Environment"},{"location":"documentation/getting-started/#home_directoryhome-directory-permissions","text":"By default, user home directories and /scratch directories are closed (permissions 700) with a parent directory setting that prevents users from opening up the permissions. See the File and Directory Access Control List page ( https://bluewaters.ncsa.illinois.edu/facl ) for Blue Waters file system policies. The /projects file system is designed as common space for your group; if you want a space that all your group members can access, that\\'s a good place for it. As always, your space on the /scratch file system is the best place for job inputs and outputs.","title":"[]{#Home_Directory}Home Directory Permissions"},{"location":"documentation/getting-started/#buildingbuilding-executables","text":"The compilers are defined in the PrgEnv module for each family of compilers. Invoke the ftn, cc, or CC (Fortran, C, or C++, respectively) commands after loading the appropriate programming environment, and the underlying compilers will be employed. To see the options specific to a compiler (pgi vs. cray) consult the man pages for the vendor\\'s compiler (man pgf90 or man crayftn). The correct xt-libsci providing the lapack and other libs for your programming environment is automatically included in the PrgEnv module you choose. The default programming environment (module PrgEnv-cray ) invokes the Cray compilers. PGI compilers are available via the PrgEnv-pgi module.Intel compilers are available via the PrgEnv-intel module. Gnu compilers are available via the PrgEnv-gnu module. OpenACC support is provided by both the Cray and PGI compilers. Please see the Compiling section of the User Guide for more information.","title":"[]{#Building}Building Executables"},{"location":"documentation/getting-started/#runningrunning-batch-jobs","text":"The batch environment is Torque/MOAB from Adaptive Computing which talk to the Cray\\'s Application Level Placement Scheduler (ALPS) to obtain resource information. The aprun utility is used to start jobs on compute nodes. Its closest analogs are mpirun or mpiexec as found on many commodity clusters. Unlike clusters, the use of aprun is mandatory on Blue Waters, which is not a Linux cluster but a massively parallel system (MPP), in order to start any jobs including non-MPI ones that run on a single node. If the PBS script does not use aprun to start the application the latter will start on a service node, which is a shared resource, and that will be a violation of the usage policy. If by any reason the single-node job does not properly run when invoked with help of aprun, the alternative is to run the single-node application in CCM-mode . aprun supports options that are unique to the Cray. There are flags to set process affinity by NUMA node, control thread placement, and set memory policy between NUMA nodes in a job. See the aprun man page for more details. Some common flags are highlighted and documented in the sample batch scripts. Example scripts are located in /sw/userdoc/samplescripts/ Following is a minimalistic PBS script to start a job app.exe running under GNU environment on two XE nodes #!/bin/bash #PBS -l nodes=2:ppn=32 #PBS -l walltime=00:30:00 #PBS -N testjob . /opt/modules/default/init/bash module swap PrgEnv-cray PrgEnv-gnu cd $PBS_O_WORKDIR aprun -n 64 ./app.exe < input.dat > output.out Once you have prepared a job script (called jobscript.pbs for example), you then use the qsub command to submit the job to the resource handling and job scheduling system. $ qsub jobscript.pbs The job is now in the queue (normal queue in this case as it was not specified). Please see the Running Your Jobs section of the User Guide for more information.","title":"[]{#Running}Running Batch Jobs"},{"location":"documentation/getting-started/#quickhardwaresummaryquick-hardware-summary","text":"The Blue Waters system is a Cray XE/XK hybrid machine composed of AMD 6276 \\\"Interlagos\\\" processors and NVIDIA GK110 \\\"Kepler\\\" accelerators all connected by the Cray Gemini torus interconnect.","title":"[]{#QuickHardwareSummary}Quick Hardware Summary"},{"location":"documentation/getting-started/#system-totals","text":"Cabinets 288 Peak Performance (XE+XK) 13.24 PF System Memory (XE+XK) 1.476 PB Compute Nodes (XE+XK) 26,864 Bulldozer Cores (XE+XK) 405,248 Kepler Accelerators 4,228 Torus Dimensions 24x24x24 Usable Disk Storage 26.4 PB","title":"System Totals"},{"location":"documentation/getting-started/#_1","text":"Please see the detailed Hardware Summary page for more information.","title":""},{"location":"documentation/user-guide/","text":"[]{#BWUserDoc-BasicInfoonBlueWaters-BriefBlueWatersSystemSummary}Brief Blue Waters System Overview []{#BWUserDoc-BasicInfoonBlueWaters-Overview}Blue Waters Blue Waters is a Cray XE6/XK7 system consisting of more than 22,500 XE6 compute nodes (each containing two AMD Interlagos processors) augmented by more than 4200 XK7 compute nodes (each containing one AMD Interlagos processor and one NVIDIA GK110 \\\"Kepler\\\" accelerator) in a single Gemini interconnection fabric. This configuration enables extremely large simulations on hundreds of thousands of traditional CPUs for science and engineering discovery, while also supporting development and optimization of cutting-edge applications capable of leveraging the compute power of thousands of GPUs. []{#BWUserDoc-BasicInfoonBlueWaters-Nodes}Blue Waters Nodes Blue Waters is equipped with three node types: traditional compute nodes (XE6), accelerated compute nodes (XK7), and service nodes. Each node type is introduced below. []{#BWUserDoc-BasicInfoonBlueWaters-TraditionalComputeNodes%28XE6%29} Traditional Compute Nodes (XE6) The XE6 dual-socket nodes are populated with 2 AMD Interlagos model 6276 CPU processors (one per socket) with a nominal clock speed of at least 2.3 GHz and 64 GB of physical memory. The Interlagos architecture employs the AMD Bulldozer core design in which two integer cores share a single floating point unit. In describing Blue Waters, we refer to the Bulldozer compute unit as a single compute \\\"core\\\" and consider the Interlagos processors as having 8 (floating point) cores each, although this processor has been described elsewhere as having 16 cores. The Bulldozer core has 16KB/64KB data/instruction L1 caches, 2 MB shared L2 and instruction support for SSSE3, SSE4.1, SSE4.2, AES-NI, PCLMULQDQ , AVX, XOP, and FMA4 . Each core is able to complete up to 8 floating point operations per cycle. The architecture supports 8 cores per socket with two dies, each die containing 4 cores forming a NUMA domain. The 4 cores of a NUMA domain share an 8 MB L3 cache. Access times for shared memory on the other die in the same socket are somewhat longer than access times within the same die. See chapters 1 and 2 in the AMD document Software Optimization Guide for AMD Family 15hProcessors for additional technical information on the 62xx processor. {style=\"height:auto; width:50%\"} []{#BWUserDoc-BasicInfoonBlueWaters-GPUenabledComputeNodes%28XK7%29}GPU-enabled Compute Nodes (XK7) Accelerator nodes are equipped with one Interlagos model 6276 CPU processor and one NVIDIA GK110 \\\"Kepler\\\" accelerator K20X . The CPU acts as a host processor to the accelerator. Currently the NVIDIA accelerator will not directly interact with the Gemini interconnect so that data needs to be moved to a node containing an accelerator, and that data may be accessed by the accelerator as mapped memory, or through DMA transfers to accelerator device memory. Each XK7 node has 32 GB of system memory while the accelarator has 6 GB of memory. The Kepler GK110 implementation includes 14 Streaming Multiprocessor (SMX) units and six 64?bit memory controllers. Each of the SMX units feature 192 single?precision CUDA cores. The Kepler architecture supports a unified memory request path for loads and stores, with an L1 cache per SMX multiprocessor. Each SMX has 64 KB of on-chip memory that can be configured as 48 KB of Shared memory with 16 KB of L1 cache, or as 16 KB of shared memory with 48 KB of L1 cache. The Kepler also features 1536KB of dedicated L2 cache memory. Single-Error Correct Double-Error Detect (SECDED) ECC code is enabled by default. {style=\"height:auto; width:50%\"} {style=\"height:auto; width:50%\"} The Cray XK currently supports exclusive mode for access to the accelerator. NVIDIA\\'s Proxy (part of \\\"Hyper-Q\\\") manages access for other contexts or multiple tasks of the user application via the CRAY_CUDA_MPS (formerly CRAY_CUDA_PROXY) environment setting . []{#BWUserDoc-BasicInfoonBlueWaters-I%2FOandServiceNodes%28XIO%29}I/O and Service Nodes (XIO) Each Cray XE6/XK6 XIO blade contains four service nodes. Service nodes use AMD Opteron \\\"Istanbul\\\" six-core processors, with 16 GB of DDR2 memory, and the same Gemini interconnect processors as compute nodes. XIO nodes take one of four roles as Service nodes: esLogin Nodes: These nodes provide user access and log in services for the system. Login nodes are network attached esLogin servers. Users access the system through these nodes, each running the full Linux operating system. PBS MOM Nodes : These are nodes which run the jobs (where the PBS script gets executed and aprun is launched) and where interactive batch jobs (qsub -I) place a user. These nodes are in the Gemini high speed network (HSN). Network Service Node: Each Network service node contains one Infiniband QDR IB card that can be connected to external networking connections. LNET Router Nodes: Manage file system metadata and transfer data to and from storage devices and applications. These nodes use Infiniband QDR cards as the interface to the storage network. Boot Node/System Database Node (SDB): Each Cray XE6/XK7 system requires one or more boot node and one or more SDB node. These nodes contain one 4-Gbit FC HBA and one GigE PCIe card. The FC HBA connects to the RAID and the GigE card connects to the System Management Workstation of the HSS. A redundant boot and SDB node are also configured to provide resiliency. []{#BWUserDoc-BasicInfoonBlueWaters-Interconnect}Interconnect The Blue Waters system employs the Cray Gemini interconnect, which implements a 3D torus topology. Note that there are 2 compute nodes per gemini hub. The torus is periodic or re-entrant in all directions (paths wrap around). A schematic of the general use of the high speed network (HSN) is shown in the following figure detailing a simplified view of the location of all node types in the torus. In reality the IO and service nodes are not confined to a single \\\"plane\\\" in the torus. {style=\"height:auto; width:50%\"} The (x,y,z) locations of the Gemini routers can be plotted to show a particular view of the torus. In the image below the Blue Waters torus is shown with gray dots representing the location of XE nodes, red spheres representing the location of XK nodes and blue spheres representing service nodes. Note that the system should be viewed as periodic or re-entrant and that there is no unique origin and no corners in a torus. {style=\"height:auto; width:50%\"} The native messaging protocol for the interconnect is Cray\\'s Generic Network Interface (GNI). A second messaging protocol, the Distributed Shared Memory Application (DMAPP) interface, is also supported. GNI and DMAPP provide low-level communication services to user-space software. GNI directly exposes the communications capabilities of the Gemini while DMAPP supports a logically shared, distributed memory (DM) programming model and provides remote memory access (RMA) between processes within a job in a one-sided manner. For more information please see the Using the GNI and DMAPP APIs document. []{#BWUserDoc-BasicInfoonBlueWaters-DataStorage}Data Storage []{#BWUserDoc-BasicInfoonBlueWaters-Onlinestorage}Online storage Blue Waters provides three different file systems built with the Lustre file system technology. The three file systems are provided to the users as follows: /u b\u0001\u0013 storage for home directories /projects b\u0001\u0013 storage for project home directories /scratch b\u0001\u0013 high performance, high capacity transient storage for applications All three file systems on Blue Waters are built using Cray Sonexion 1600 Lustre appliances. The Cray Sonexion 1600 appliances provide the basic storage building block for the Blue Waters I/O architecture and are referred to as a \\\"Scalable Storage Unit\\\" (SSU). Each SSU is RAID protected and is capable of providing up to 5.35 GB/s of IO performance and \\~120TB of usable disk space. The /u and /projects are configured with 18 (eighteen) SSUs each, to provide 2 PB usable storage and 96 GB/s IO performance. The /scratch file system uses 180 (one hundred eighty) SSUs to provide 21.6PB of usable disk storage and 963 GB/s IO performance. This file system can provide storage for up to 2 million file system objects. Permissions for /home , /project/sciteam/psn and /scratch/sciteam/user are set to prevent unintentional sharing. Please see the File and Directory Access Control List page for more information. Sharing by grpup in /project/sciteam/psn is also available in scratch as /scratch/sciteam/PREFIX_psn where prefix will be one of EOT , GS , ILL . Please contact \\ []{#BWUserDoc-BasicInfoonBlueWaters-DataStorage}Usage Guidelines Blue Waters uses a one-time password system for logging into accounts based on Duo Moble. You will need to either install the Duo app on a smart phone or request a Duo hardtoken during the identity creation process. . Queues and quotas discussed in the User Guide. []{#BWUserDoc-BasicInfoonBlueWaters-DataStorage}Compiling and Linking The Blue Waters development environment include four compiler suites: Cray Compilation Environment (CCE) : PrgEnv-cray PGI Compiler Suite : PrgEnv-pgi Intel Compilers : PrgEnv-intel GNU Compilers : PrgEnv-gnu The default programming environment is the Cray ( PrgEnv-cray ) compiler suite. Other compilers may be selected using the module swap command with either PrgEnv-pgi or PrgEnv-gnu . The programming environments provide wrappers for compiling Fortran (ftn), C (cc), or C++ (CC) programs, so that similar compiler commands can be used no matter which compiler environment is selected. These wrapper scripts also check for the presense of other loaded modules and take the appropriate measures to add include paths and libraries with compiling and linking. See the Compiling page in the User Guide for more information. []{#Reference}Reference Links to Cray, AMD, Nvidia & compiler vendor developer sites with a list of recommended reading: Processor architecture Software Optimization Guide for AMD Family 15h Processors AMD64 Architecture Programmer\\'s Manual Volume 1: Application Programming AMD64 Architecture Programmer\\'s Manual Volume 2: System Programming AMD64 Architecture Programmer\\'s Manual Volume 3: General-Purpose and System Instructions AMD64 Architecture Programmer\\'s Manual Volume 4: 128-Bit and 256-Bit Media Instructions AMD64 Architecture Programmer\\'s Manual Volume 5: 64-Bit Media and x87 Floating Point Instructions <!-- --> Accelerator architecture NVIDIA Kepler GK110 Architecture Whitepaper Nvidia Kepler K20X Board Design NVIDIA K20 and K20X Performance Technical Brief I nside Kepler NVIDIA Tesla Kepler Family Overview PGI Accelerator Compilers [OMP-like directives] PGI CUDA Fortran Nvidia CUDA Toolkit c, c++, math libs CUDA C Programming Guide CUDA C Best Practices (ch. 4 config. optimization tips , ch. 8 multi-gpu and MPI tips, see appendix A for recommendations) Interconnect (gemini) related docs Gemini-based Cielo performance evaluation at Sandia Optimizing Global Arrays for Gemini Compiler documentation Cray Application Developer\\'s Environment User\\'s Guide: Using Compilers Cray C and C++ Reference Manual AMD Compiler Options Quick Reference Guide for Valencia and Interlagos Refer to http://developer.amd.com/tools/open64/Pages/default.aspx for latest information on AMD compilers.","title":"User Guide"},{"location":"documentation/user-guide/#bwuserdoc-basicinfoonbluewaters-briefbluewaterssystemsummarybrief-blue-waters-system-overview","text":"","title":"[]{#BWUserDoc-BasicInfoonBlueWaters-BriefBlueWatersSystemSummary}Brief Blue Waters System Overview"},{"location":"documentation/user-guide/#bwuserdoc-basicinfoonbluewaters-overviewblue-waters","text":"Blue Waters is a Cray XE6/XK7 system consisting of more than 22,500 XE6 compute nodes (each containing two AMD Interlagos processors) augmented by more than 4200 XK7 compute nodes (each containing one AMD Interlagos processor and one NVIDIA GK110 \\\"Kepler\\\" accelerator) in a single Gemini interconnection fabric. This configuration enables extremely large simulations on hundreds of thousands of traditional CPUs for science and engineering discovery, while also supporting development and optimization of cutting-edge applications capable of leveraging the compute power of thousands of GPUs.","title":"[]{#BWUserDoc-BasicInfoonBlueWaters-Overview}Blue Waters"},{"location":"documentation/user-guide/#bwuserdoc-basicinfoonbluewaters-nodesblue-waters-nodes","text":"Blue Waters is equipped with three node types: traditional compute nodes (XE6), accelerated compute nodes (XK7), and service nodes. Each node type is introduced below. []{#BWUserDoc-BasicInfoonBlueWaters-TraditionalComputeNodes%28XE6%29} Traditional Compute Nodes (XE6) The XE6 dual-socket nodes are populated with 2 AMD Interlagos model 6276 CPU processors (one per socket) with a nominal clock speed of at least 2.3 GHz and 64 GB of physical memory. The Interlagos architecture employs the AMD Bulldozer core design in which two integer cores share a single floating point unit. In describing Blue Waters, we refer to the Bulldozer compute unit as a single compute \\\"core\\\" and consider the Interlagos processors as having 8 (floating point) cores each, although this processor has been described elsewhere as having 16 cores. The Bulldozer core has 16KB/64KB data/instruction L1 caches, 2 MB shared L2 and instruction support for SSSE3, SSE4.1, SSE4.2, AES-NI, PCLMULQDQ , AVX, XOP, and FMA4 . Each core is able to complete up to 8 floating point operations per cycle. The architecture supports 8 cores per socket with two dies, each die containing 4 cores forming a NUMA domain. The 4 cores of a NUMA domain share an 8 MB L3 cache. Access times for shared memory on the other die in the same socket are somewhat longer than access times within the same die. See chapters 1 and 2 in the AMD document Software Optimization Guide for AMD Family 15hProcessors for additional technical information on the 62xx processor. {style=\"height:auto; width:50%\"} []{#BWUserDoc-BasicInfoonBlueWaters-GPUenabledComputeNodes%28XK7%29}GPU-enabled Compute Nodes (XK7) Accelerator nodes are equipped with one Interlagos model 6276 CPU processor and one NVIDIA GK110 \\\"Kepler\\\" accelerator K20X . The CPU acts as a host processor to the accelerator. Currently the NVIDIA accelerator will not directly interact with the Gemini interconnect so that data needs to be moved to a node containing an accelerator, and that data may be accessed by the accelerator as mapped memory, or through DMA transfers to accelerator device memory. Each XK7 node has 32 GB of system memory while the accelarator has 6 GB of memory. The Kepler GK110 implementation includes 14 Streaming Multiprocessor (SMX) units and six 64?bit memory controllers. Each of the SMX units feature 192 single?precision CUDA cores. The Kepler architecture supports a unified memory request path for loads and stores, with an L1 cache per SMX multiprocessor. Each SMX has 64 KB of on-chip memory that can be configured as 48 KB of Shared memory with 16 KB of L1 cache, or as 16 KB of shared memory with 48 KB of L1 cache. The Kepler also features 1536KB of dedicated L2 cache memory. Single-Error Correct Double-Error Detect (SECDED) ECC code is enabled by default. {style=\"height:auto; width:50%\"} {style=\"height:auto; width:50%\"} The Cray XK currently supports exclusive mode for access to the accelerator. NVIDIA\\'s Proxy (part of \\\"Hyper-Q\\\") manages access for other contexts or multiple tasks of the user application via the CRAY_CUDA_MPS (formerly CRAY_CUDA_PROXY) environment setting . []{#BWUserDoc-BasicInfoonBlueWaters-I%2FOandServiceNodes%28XIO%29}I/O and Service Nodes (XIO) Each Cray XE6/XK6 XIO blade contains four service nodes. Service nodes use AMD Opteron \\\"Istanbul\\\" six-core processors, with 16 GB of DDR2 memory, and the same Gemini interconnect processors as compute nodes. XIO nodes take one of four roles as Service nodes: esLogin Nodes: These nodes provide user access and log in services for the system. Login nodes are network attached esLogin servers. Users access the system through these nodes, each running the full Linux operating system. PBS MOM Nodes : These are nodes which run the jobs (where the PBS script gets executed and aprun is launched) and where interactive batch jobs (qsub -I) place a user. These nodes are in the Gemini high speed network (HSN). Network Service Node: Each Network service node contains one Infiniband QDR IB card that can be connected to external networking connections. LNET Router Nodes: Manage file system metadata and transfer data to and from storage devices and applications. These nodes use Infiniband QDR cards as the interface to the storage network. Boot Node/System Database Node (SDB): Each Cray XE6/XK7 system requires one or more boot node and one or more SDB node. These nodes contain one 4-Gbit FC HBA and one GigE PCIe card. The FC HBA connects to the RAID and the GigE card connects to the System Management Workstation of the HSS. A redundant boot and SDB node are also configured to provide resiliency.","title":"[]{#BWUserDoc-BasicInfoonBlueWaters-Nodes}Blue Waters Nodes"},{"location":"documentation/user-guide/#bwuserdoc-basicinfoonbluewaters-interconnectinterconnect","text":"The Blue Waters system employs the Cray Gemini interconnect, which implements a 3D torus topology. Note that there are 2 compute nodes per gemini hub. The torus is periodic or re-entrant in all directions (paths wrap around). A schematic of the general use of the high speed network (HSN) is shown in the following figure detailing a simplified view of the location of all node types in the torus. In reality the IO and service nodes are not confined to a single \\\"plane\\\" in the torus. {style=\"height:auto; width:50%\"} The (x,y,z) locations of the Gemini routers can be plotted to show a particular view of the torus. In the image below the Blue Waters torus is shown with gray dots representing the location of XE nodes, red spheres representing the location of XK nodes and blue spheres representing service nodes. Note that the system should be viewed as periodic or re-entrant and that there is no unique origin and no corners in a torus. {style=\"height:auto; width:50%\"} The native messaging protocol for the interconnect is Cray\\'s Generic Network Interface (GNI). A second messaging protocol, the Distributed Shared Memory Application (DMAPP) interface, is also supported. GNI and DMAPP provide low-level communication services to user-space software. GNI directly exposes the communications capabilities of the Gemini while DMAPP supports a logically shared, distributed memory (DM) programming model and provides remote memory access (RMA) between processes within a job in a one-sided manner. For more information please see the Using the GNI and DMAPP APIs document.","title":"[]{#BWUserDoc-BasicInfoonBlueWaters-Interconnect}Interconnect"},{"location":"documentation/user-guide/#bwuserdoc-basicinfoonbluewaters-datastoragedata-storage","text":"[]{#BWUserDoc-BasicInfoonBlueWaters-Onlinestorage}Online storage Blue Waters provides three different file systems built with the Lustre file system technology. The three file systems are provided to the users as follows: /u b\u0001\u0013 storage for home directories /projects b\u0001\u0013 storage for project home directories /scratch b\u0001\u0013 high performance, high capacity transient storage for applications All three file systems on Blue Waters are built using Cray Sonexion 1600 Lustre appliances. The Cray Sonexion 1600 appliances provide the basic storage building block for the Blue Waters I/O architecture and are referred to as a \\\"Scalable Storage Unit\\\" (SSU). Each SSU is RAID protected and is capable of providing up to 5.35 GB/s of IO performance and \\~120TB of usable disk space. The /u and /projects are configured with 18 (eighteen) SSUs each, to provide 2 PB usable storage and 96 GB/s IO performance. The /scratch file system uses 180 (one hundred eighty) SSUs to provide 21.6PB of usable disk storage and 963 GB/s IO performance. This file system can provide storage for up to 2 million file system objects. Permissions for /home , /project/sciteam/psn and /scratch/sciteam/user are set to prevent unintentional sharing. Please see the File and Directory Access Control List page for more information. Sharing by grpup in /project/sciteam/psn is also available in scratch as /scratch/sciteam/PREFIX_psn where prefix will be one of EOT , GS , ILL . Please contact \\ []{#BWUserDoc-BasicInfoonBlueWaters-DataStorage}Usage Guidelines Blue Waters uses a one-time password system for logging into accounts based on Duo Moble. You will need to either install the Duo app on a smart phone or request a Duo hardtoken during the identity creation process. . Queues and quotas discussed in the User Guide.","title":"[]{#BWUserDoc-BasicInfoonBlueWaters-DataStorage}Data Storage"},{"location":"documentation/user-guide/#bwuserdoc-basicinfoonbluewaters-datastoragecompiling-and-linking","text":"The Blue Waters development environment include four compiler suites: Cray Compilation Environment (CCE) : PrgEnv-cray PGI Compiler Suite : PrgEnv-pgi Intel Compilers : PrgEnv-intel GNU Compilers : PrgEnv-gnu The default programming environment is the Cray ( PrgEnv-cray ) compiler suite. Other compilers may be selected using the module swap command with either PrgEnv-pgi or PrgEnv-gnu . The programming environments provide wrappers for compiling Fortran (ftn), C (cc), or C++ (CC) programs, so that similar compiler commands can be used no matter which compiler environment is selected. These wrapper scripts also check for the presense of other loaded modules and take the appropriate measures to add include paths and libraries with compiling and linking. See the Compiling page in the User Guide for more information.","title":"[]{#BWUserDoc-BasicInfoonBlueWaters-DataStorage}Compiling and Linking"},{"location":"documentation/user-guide/#referencereference","text":"Links to Cray, AMD, Nvidia & compiler vendor developer sites with a list of recommended reading: Processor architecture Software Optimization Guide for AMD Family 15h Processors AMD64 Architecture Programmer\\'s Manual Volume 1: Application Programming AMD64 Architecture Programmer\\'s Manual Volume 2: System Programming AMD64 Architecture Programmer\\'s Manual Volume 3: General-Purpose and System Instructions AMD64 Architecture Programmer\\'s Manual Volume 4: 128-Bit and 256-Bit Media Instructions AMD64 Architecture Programmer\\'s Manual Volume 5: 64-Bit Media and x87 Floating Point Instructions <!-- --> Accelerator architecture NVIDIA Kepler GK110 Architecture Whitepaper Nvidia Kepler K20X Board Design NVIDIA K20 and K20X Performance Technical Brief I nside Kepler NVIDIA Tesla Kepler Family Overview PGI Accelerator Compilers [OMP-like directives] PGI CUDA Fortran Nvidia CUDA Toolkit c, c++, math libs CUDA C Programming Guide CUDA C Best Practices (ch. 4 config. optimization tips , ch. 8 multi-gpu and MPI tips, see appendix A for recommendations) Interconnect (gemini) related docs Gemini-based Cielo performance evaluation at Sandia Optimizing Global Arrays for Gemini Compiler documentation Cray Application Developer\\'s Environment User\\'s Guide: Using Compilers Cray C and C++ Reference Manual AMD Compiler Options Quick Reference Guide for Valencia and Interlagos Refer to http://developer.amd.com/tools/open64/Pages/default.aspx for latest information on AMD compilers.","title":"[]{#Reference}Reference"},{"location":"documentation/user-guide/running-your-jobs/","text":"{style=\"height:450px; width:600px\"} The Blue Waters system uses TORQUE Resource Manager integrated with the Moab Workload Manager to schedule and manage user jobs. Since Torque is based on OpenPBS, most of the commands for managing your jobs on Blue Waters will be the same as PBS commands. The batch system works with the following components to start a job Application launcher (aprun) utility launches applications on compute nodes (in case you\\'re used to using mpirun on other systems to launch jobs, the aprun command takes its place on the Cray system). Application Level Placement Scheduler (ALPS) has applications submitted to it by aprun for placement and execution. The basic process to execute a job on Blue Waters is as follows: Determine the resources needed for your job Pick a queue that will provide the required resources Create a job script that includes the aprun command and describes the resources needed for your job Verify that your job is setup for checkpointing (at the application level) if running at scale or for long wall time Submit the job script to the batch system using qsub You can set the enviroment variable \\'NOAPRUNWARN=1\\' in the job submission environment to suppress the message: \\\"WARNING: A scan of your job script did not find an invocation of \\'aprun\\'/\\'ccmrun\\'. This may be due to using another command to invoke \\'aprun\\'/\\'ccmrun\\', or the use of a shell variable. Job will be submitted as usual, but please ensure your job script eventually invokes \\'aprun\\'/\\'ccmrun\\' command to execute tasks on allocated compute nodes.\\\" You can see the options for qsub by looking at the qsub man page. To delete a job from the queue before it runs, or after it\\'s begun running, use \\\"qdel \\ \\\".","title":"Running Your Jobs"},{"location":"documentation/user-guide/running-your-jobs/#styleheight450px-width600px","text":"The Blue Waters system uses TORQUE Resource Manager integrated with the Moab Workload Manager to schedule and manage user jobs. Since Torque is based on OpenPBS, most of the commands for managing your jobs on Blue Waters will be the same as PBS commands. The batch system works with the following components to start a job Application launcher (aprun) utility launches applications on compute nodes (in case you\\'re used to using mpirun on other systems to launch jobs, the aprun command takes its place on the Cray system). Application Level Placement Scheduler (ALPS) has applications submitted to it by aprun for placement and execution. The basic process to execute a job on Blue Waters is as follows: Determine the resources needed for your job Pick a queue that will provide the required resources Create a job script that includes the aprun command and describes the resources needed for your job Verify that your job is setup for checkpointing (at the application level) if running at scale or for long wall time Submit the job script to the batch system using qsub You can set the enviroment variable \\'NOAPRUNWARN=1\\' in the job submission environment to suppress the message: \\\"WARNING: A scan of your job script did not find an invocation of \\'aprun\\'/\\'ccmrun\\'. This may be due to using another command to invoke \\'aprun\\'/\\'ccmrun\\', or the use of a shell variable. Job will be submitted as usual, but please ensure your job script eventually invokes \\'aprun\\'/\\'ccmrun\\' command to execute tasks on allocated compute nodes.\\\" You can see the options for qsub by looking at the qsub man page. To delete a job from the queue before it runs, or after it\\'s begun running, use \\\"qdel \\ \\\".","title":"{style=\"height:450px; width:600px\"}"}]}